# QBM Codebase Assessment (Complete Inventory + Utilization Map)

This is a **single, consolidated report** of what exists in `d:\Quran_matrix` (code, scripts, data, frontends, tests, CI), how the pieces connect, and what is currently **used vs underused** in the production/benchmark path.

Generated: 2025-12-28

---

## 0) Executive Summary (what you have, in one page)

QBM is already a **multi-layer, evidence-first system** with:
- A FastAPI backend (`src/api/main.py`) exposing both dataset APIs and a canonical proof contract endpoint (`/api/proof/query`) built around `schemas/proof_response_v2.py`.
- A deterministic **Truth Metrics** artifact (`data/metrics/truth_metrics_v1.json`) generated by `scripts/build_truth_metrics_v1.py` and served verbatim at `/api/metrics/overview` for UI + audits.
- A deterministic **benchmark harness** (`scripts/run_qbm_benchmark.py` + `src/benchmarks/scoring.py`) driven by `data/benchmarks/qbm_legendary_200.v1.jsonl` that emits reproducible JSON/MD/CSV + per-failure artifacts.
- A “Tier A / proof_only” path (fast, CI-safe) plus a “FullPower” path (index-backed, optional GPU).
- Two Next.js frontends (`qbm-frontend/` and `qbm-frontendv3/`) with Playwright E2E coverage (v3).

The main opportunity is **not “missing features”**; it’s **consolidation and consistent wiring**:
- Clarify/standardize canonical data paths referenced by docs vs actual files.
- Reduce parallel “old vs new” stacks (`src/ai/*` vs `src/ml/*`) by declaring which is canonical per endpoint.
- Integrate taxonomy (11-axis/profile) modules into the proof path for PROFILE-style benchmark questions.
- Fix repo hygiene/encoding issues that waste time (generated dirs, reserved filenames, mixed encodings).

---

## 1) Critical Entry Points (what to run / where the system starts)

### Backend API
- App: `src/api/main.py` (`uvicorn src.api.main:app --reload`)
- Canonical proof endpoint: `src/api/routers/proof.py` → `POST /api/proof/query`
- Truth metrics endpoint: `GET /api/metrics/overview` (served from `data/metrics/truth_metrics_v1.json`)
- Discovery endpoints: `src/api/discovery_routes.py` → `/discovery/*` (uses `src/ai/discovery/*`)

### Benchmark (deterministic harness)
- Dataset: `data/benchmarks/qbm_legendary_200.v1.jsonl`
- Runner: `scripts/run_qbm_benchmark.py`
- Scoring logic: `src/benchmarks/scoring.py`
- Output folder: `reports/benchmarks/` (plus `reports/benchmarks/failures/<timestamp>/`)

### Frontend
- Current “operational” UI (per `docs/RUN_LOCAL.md`): `qbm-frontendv3/` (`npm run dev`)
- Legacy UI: `qbm-frontend/`
- E2E: `qbm-frontendv3/e2e/qbm.spec.ts`

---

## 2) System Modes (what runs in CI vs what runs locally)

### Mode A: proof_only (Tier A / CI-safe)
Used by: CI benchmark smoke, fast debugging, deterministic retrieval checks.

Path:
- `POST /api/proof/query` with `"proof_only": true`
- `src/api/routers/proof.py` routes to `src/ml/proof_only_backend.py`
- Evidence source: `data/evidence/evidence_index_v2_chunked.jsonl` (chunked JSONL substrate)

Primary value:
- Validates **contract stability** and **no-fabrication** without GPU/LLM.

### Mode B: FullPower (index-backed)
Used by: interactive deep queries, richer ranking, optional GPU acceleration.

Path:
- `POST /api/proof/query` with `"proof_only": false`
- `src/api/routers/proof.py` initializes `src/ml/full_power_system.py` and wraps it via `src/ml/mandatory_proof_system.py`
- Index: `data/indexes/full_power_index.npy` (plus metadata)

Guards:
- `QBM_FULLPOWER_READY=1` controls “build index on first request” behavior.

### Mode C: Discovery (semantic exploration endpoints)
Used by: `/discovery/search`, co-occurrence patterns, clustering.

Path:
- `src/api/discovery_routes.py` lazily initializes `src/ai/discovery/*` components.

Note: this stack is **separate** from the canonical proof contract path (see “Parallel stacks” under gaps/risks).

---

## 3) Codebase Map (what exists, by directory)

### `src/api/` (21 Python files)
Purpose: FastAPI app + routers + legacy endpoints + unified “brain/graph” helpers.

Key files:
- `src/api/main.py`: main app, CORS/rate limit, legacy endpoints, truth metrics endpoint, router includes
- `src/api/routers/proof.py`: canonical proof endpoint (proof_only + FullPower + CROSS_CONTEXT_BEHAVIOR fast-path)
- `src/api/routers/genome.py`: Q25 genome export endpoints
- `src/api/routers/reviews.py`: scholar review workflow API
- `src/api/discovery_routes.py`: discovery APIs that use `src/ai/discovery/*`

### `src/ml/` (46 Python files)
Purpose: canonical proof machinery + retrieval + planners + optional training/eval utilities.

Core runtime path:
- `src/ml/mandatory_proof_system.py`: assembles proof bundle and debug fields; integrates LegendaryPlanner for benchmark intents
- `src/ml/proof_only_backend.py`: deterministic substrate for proof_only mode
- `src/ml/intent_classifier.py`: deterministic benchmark-intent classifier (Sections A–J style)
- `src/ml/query_router.py`: deterministic query routing (AYAH_REF, SURAH_REF, CONCEPT_REF, FREE_TEXT, CROSS_CONTEXT_BEHAVIOR)
- `src/ml/legendary_planner.py`: 25-class planner used for benchmark intents (both proof_only + full path)
- `src/ml/stratified_retriever.py`, `src/ml/hybrid_evidence_retriever.py`: retrieval strategies

Present but not fully wired into proof path yet (high value):
- `src/ml/qbm_bouzidani_taxonomy.py`, `src/ml/qbm_usul_taxonomy.py`, `src/ml/quranic_behavior_taxonomy.py`: taxonomy modules for profile/axes outputs
- `src/ml/graph_reasoner.py`: PyG-optional graph reasoning layer
- `src/ml/routed_evidence_retriever.py`: unified router+retriever (currently defined but not used in production endpoint)
- `src/ml/query_planner.py`: “Phase 7.1 planner” but currently hardwired to **v1** artifacts (see gaps)

### `src/ai/` (26 Python files)
Purpose: parallel AI stack: Chroma vector store, RAG pipeline, ontology, plus discovery submodules.

Used today by:
- `src/api/discovery_routes.py` (semantic search/pattern discovery/cross-reference/clustering)

Not used by the canonical proof contract path (unless explicitly integrated):
- Chroma-based vector store (`src/ai/vectors/qbm_vectors.py`) and RAG (`src/ai/rag/qbm_rag.py`) exist but are not the benchmark/proof-only substrate.

### `scripts/` (29 Python files)
Purpose: operational scripts (build indexes/graphs, audits, training, benchmark).

See Section 5 for full catalog.

### `tools/` (34 Python files) + `label_studio/` (assets)
Purpose: annotation workflow tooling (Label Studio, pilot generation, tafsir download/dedupe, validation helpers).

### `schemas/`
Purpose: hard contracts and schemas.
- `schemas/proof_response_v2.py`: canonical API contract validation
- `schemas/postgres_truth_layer.sql`: DB schema for truth layer export

### `tests/`
Purpose: contract + invariants, plus Tier A/Tier B style structure.
- `pytest.ini` defines markers and excludes heavy dirs.
- `.github/workflows/ci.yml` runs Tier A tests + benchmark smoke on push/PR.

---

## 4) Data & Artifacts Inventory (what exists on disk)

### Data directory top-level (observed)
`data/annotations`, `data/benchmarks`, `data/evidence`, `data/exports`, `data/graph`, `data/indexes`, `data/metrics`, `data/models`, `data/quran`, `data/tafsir`, `data/evaluation`, `data/test_fixtures`, …

### Key “system required” artifacts (present/absent)
| Artifact | Path | Present | Used by |
|---|---|---:|---|
| Canonical entities | `vocab/canonical_entities.json` | ✓ | entity resolution, genome, scoring |
| Entity typing map | `vocab/entity_types.json` | ✓ | `src/ml/query_router.py` |
| 7 tafsir sources | `data/tafsir/*.ar.jsonl` | ✓ | deterministic retrieval + comparisons |
| Chunked evidence substrate | `data/evidence/evidence_index_v2_chunked.jsonl` | ✓ | proof_only + deterministic retrieval |
| Concept index v2 | `data/evidence/concept_index_v2.jsonl` | ✓ | planner + scoring + concept coverage |
| Semantic graph v2 | `data/graph/semantic_graph_v2.json` | ✓ | planner + graph endpoints |
| Co-occurrence graph | `data/graph/cooccurrence_graph_v1.json` | ✓ | discovery-only (not causal) |
| FullPower index | `data/indexes/full_power_index.npy` | ✓ | FullPower mode |
| Truth metrics artifact | `data/metrics/truth_metrics_v1.json` | ✓ | `/api/metrics/overview` + UI |
| “quran_uthmani.json” (docs mention) | `data/quran/quran_uthmani.json` | ✗ | docs mismatch (see gaps) |
| Tokenized Quran contract | `data/quran/uthmani_hafs_v1.tok_v1.json` | ✓ | verse text lookup (proof_only) |

### Data volume (high-level)
| Folder | Files | Approx size |
|---|---:|---:|
| `data/models/` | 266 | ~44 GB |
| `data/indexes/` | 9 | ~493 MB |
| `data/evidence/` | 8 | ~232 MB |
| `data/tafsir/` | 10 | ~158 MB |
| `data/annotations/` | 134 | ~200 MB |

---

## 5) Script Catalog (everything under `scripts/`)

### Index/graph building
- `scripts/build_chunked_evidence_index.py`: Build chunked evidence index
- `scripts/build_concept_evidence_index.py`: Deterministic concept evidence index (Phase 6.1)
- `scripts/build_concept_evidence_index_v2.py`: Complete concept evidence index v2 (Phase 8.2)
- `scripts/build_evidence_index.py`: Deterministic evidence index
- `scripts/build_evidence_backed_graphs.py`: Evidence-backed graphs (Phase 6.3)
- `scripts/build_semantic_graph_v2.py`: Semantic graph v2 (Phase 8.3)
- `scripts/build_test_fixture_index.py`: CPU-only fixture evidence index for tests

### Deterministic truth artifacts
- `scripts/build_truth_metrics_v1.py`: Builds `data/metrics/truth_metrics_v1.json` (UI + audits)

### Benchmark + gates
- `scripts/run_qbm_benchmark.py`: Runs JSONL dataset against FastAPI TestClient and writes reports
- `scripts/test_release_gate.py`: “must-pass” release test runner wrapper

### Validation/audit/forensics
- `scripts/validate_foundation_data.py`: baseline data validation (Day 1)
- `scripts/comprehensive_data_audit.py`: comprehensive audit checks
- `scripts/check_concept_coverage.py`: concept map completeness check
- `scripts/compute_real_metrics.py`: compute metrics from actual files (sanity against “fake” stats)
- `scripts/forensic_embedding_check.py`: embedding forensic checks
- `scripts/generate_baseline_report.py`: generates Phase 0 baseline report
- `scripts/check_pyg_health.py`: PyG readiness probe (safe opt-in)
- `scripts/check_torch_pyg_versions.py`: prints torch/PyG version matrix
- `scripts/check_tafsir_db.py`: quick SQLite tafsir DB structure check

### Cleaning
- `scripts/clean_all_tafsir.py`: tafsir cleaning (Phase 3)
- `scripts/clean_and_dedupe_tafsir.py`: dedupe newly downloaded tafsir
- `scripts/clean_tafsir_jsonl_inplace.py`: in-place JSONL cleaning

### Training / heavy compute
- `scripts/train_layer2_embeddings.py`: Layer 2 embeddings training
- `scripts/train_layer2_multi_gpu.py`: multi-GPU embeddings training
- `scripts/run_phase5_extraction.py`: Phase 5 extraction from tafsir sources
- `scripts/run_phase6_gpu.py`: GPU accelerated batch processing
- `scripts/run_phase7_discovery.py`: discovery system run
- `scripts/load_truth_layer_to_postgres.py`: exports truth layer to Postgres (Phase 8.4)
- `scripts/test_pattern_fix.py`: pattern discovery fix test

---

## 6) Additional Script Catalogs (`src/scripts/` + `tools/`)

### `src/scripts/` (15 scripts)
These are mostly “data/annotation workflow” utilities (legacy-ish, but still valuable).
- `src/scripts/analyze_qbm_data.py`: dataset analysis
- `src/scripts/analyze_surah_coverage.py`: coverage analysis by surah
- `src/scripts/annotate_batch_expert.py`: annotate a single batch file
- `src/scripts/calculate_iaa.py`: compute IAA across annotators
- `src/scripts/check_coverage.py`: coverage checks
- `src/scripts/coverage_audit.py`: identify gaps to create batches
- `src/scripts/expand_pilot_100.py`: pilot expansion helper
- `src/scripts/export_tiers.py`: tier stats/export
- `src/scripts/generate_batches.py`: batch generation
- `src/scripts/progress_report.py`: console progress reporting
- `src/scripts/quality_check.py`: span readiness checks
- `src/scripts/setup_database.py`: initialize coverage tracker
- `src/scripts/test_academic_query.py`: interactive research query
- `src/scripts/test_academic_query_v2.py`: research query run + save results
- `src/scripts/test_behavior_search.py`: behavior search test

### `tools/` (34 scripts)
These are “operator tools” (Label Studio + pilot + tafsir + validation).

**Label Studio integration** (`tools/label_studio/*`)
- End-to-end automation: create tasks, export annotations, convert exports to QBM format, and apply deterministic fixups.

**Pilot workflow** (`tools/pilot/*`)
- Build pilot selections and Label Studio import tasks from local Quran XML/tokenization sources.

**Tafsir ingestion** (`tools/tafsir/*`)
- Download tafsir, dedupe, setup DB, and run local lookup/snippet extraction.

**Quran ingestion** (`tools/quran/*`)
- Convert full tokenization JSON into repo’s minimal contract format.

**Validation** (`tools/validation/*`)
- Disagreement analysis and gold comparisons.

---

## 7) Benchmark Remediation Loop (what’s already implemented)

Docs: `docs/BENCHMARK_REMEDIATION.md`

Mechanics:
- Runner: `scripts/run_qbm_benchmark.py`
- Schema contract: `schemas/proof_response_v2.py` (`validate_response_against_contract()` is used in the runner)
- Scoring: `src/benchmarks/scoring.py` includes “global FAIL” invariants like:
  - no fallback for structured intents
  - no missing provenance on emitted tafsir chunks
  - no generic opening-verse defaults
  - no % claims without derivation fields

Outputs:
- `reports/benchmarks/<dataset>_<timestamp>.json|.md|.csv`
- `reports/benchmarks/failures/<timestamp>/<id>.json` includes repro snippets + exact request/response

### Current benchmark snapshot (latest smoke report found)
From `reports/benchmarks/qbm_legendary_200_20251228_194544Z.json` (20-item smoke):
- PASS: 4
- PARTIAL: 12
- FAIL: 4
- Schema invalid: 0
- Top fail reasons: `generic_opening_verses_default` (3), `fallback_used_for_structured_intent` (1)

---

## 8) Test Suite + CI Map (what runs where)

### Test inventory
Primary tests live in `tests/` (contract, retrieval invariants, graph audits, security, Tier A/B structure).

Key enforcement points:
- Proof contract validation: `schemas/proof_response_v2.py` + `tests/test_proof_response_schema.py`
- No-fallback + no-synthetic-evidence: `tests/test_no_fallback_invariant.py`, `tests/test_no_synthetic_evidence.py`
- Deterministic pagination invariants: `tests/test_pagination_deterministic.py`, `tests/test_proof_pagination.py`
- Truth metrics artifact correctness: `tests/test_truth_metrics_v1.py`, `tests/test_api_metrics_overview.py`

### CI workflow
`\.github/workflows/ci.yml` runs:
- Tier A tests (CPU-only) + benchmark smoke (`--smoke --proof-only --ci`)
- Ruff lint (non-blocking via `--exit-zero`)
- Windows reserved filename scan (repo hygiene)
- Frontend build + Playwright E2E (non-blocking: `continue-on-error: true`)

---

## 9) Frontend Map (what UIs exist and what they hit)

### `qbm-frontendv3/` (recommended)
Pages observed:
- `/metrics` (reads `/api/metrics/overview`)
- `/proof` (posts to `/api/proof/query`)
- `/genome` (hits `/api/genome/*`)
- `/reviews` (hits `/api/reviews/*`)
- `/explorer`, `/discovery`, `/taxonomy`, `/dashboard`, `/insights`

E2E coverage: `qbm-frontendv3/e2e/qbm.spec.ts`

### `qbm-frontend/` (legacy)
Pages observed:
- `/annotate`, `/dashboard`, `/explorer`, `/insights`, `/research`

---

## 10) What’s Underused or Inconsistent (highest-leverage gaps)

### A) Taxonomy modules not integrated into the proof endpoint
You have multiple taxonomy implementations in `src/ml/` (Bouzidani, usul-aligned, “proper behavior taxonomy”), but PROFILE-style benchmark capabilities expect `proof.taxonomy.dimensions` outputs consistently. Today, taxonomy is **not a single canonical implementation** wired into `/api/proof/query`.

### B) Parallel stacks: `src/ai/*` vs `src/ml/*`
Both exist and both are valuable, but they currently represent **two different architectures**:
- `src/ml/*` is the canonical benchmark/proof contract path (deterministic, JSONL substrate, contract enforcement).
- `src/ai/*` is used for discovery endpoints and includes Chroma-based RAG primitives.

This needs a clear “canonical vs experimental” declaration per endpoint to avoid accidental drift.

### C) `src/ml/query_planner.py` is hardwired to v1 artifacts
It references:
- `data/evidence/concept_index_v1.jsonl`
- `data/graph/semantic_graph_v1.json`

That conflicts with the v2 substrate used elsewhere (`concept_index_v2.jsonl`, `semantic_graph_v2.json`). Either upgrade it to v2 or formally deprecate it.

### D) Docs reference paths that don’t exist in the repo state
Examples observed:
- `docs/CANONICAL_DATA_PATH.md` references `data/quran/quran_uthmani.json` and `data/indices/chunked_evidence_index.json` which do not match the current on-disk artifacts (`data/quran/uthmani_hafs_v1.tok_v1.json`, `data/evidence/evidence_index_v2_chunked.jsonl`).

### E) Repo hygiene + encoding issues waste time
- Large generated folders (e.g. `node_modules/`, `.next/`) are present locally and not ignored.
- Windows reserved filename `nul` exists in repo root (untracked but hazardous).
- Several docs show mojibake-like characters; standardize file encoding (UTF-8) to prevent garbled Arabic in terminals/editors.

---

## 11) What to “Use Everything” (practical utilization checklist)

If your goal is “maximum value with what we already have”, the highest ROI is:

1) **Truth Layer is mandatory** (metrics + provenance)
   - Build metrics: `scripts/build_truth_metrics_v1.py`
   - Gate UI stats exclusively through `/api/metrics/overview`

2) **Benchmark loop is your steering wheel**
   - Iterate with `--smoke --proof-only --ci` first (fast signal)
   - Use per-failure artifacts in `reports/benchmarks/failures/*` to reproduce deterministically

3) **Route benchmark intents through LegendaryPlanner consistently**
   - Already present in both `src/ml/proof_only_backend.py` and `src/ml/mandatory_proof_system.py` for benchmark intents
   - Next improvement: make planner outputs drive taxonomy/graph payloads (not only verse selection)

4) **Adopt one canonical taxonomy output**
   - Pick one taxonomy module as canonical for PROFILE_11D and wire into `/api/proof/query`

5) **Keep PyG opt-in (already implemented)**
   - Use `scripts/check_pyg_health.py` before setting `QBM_ENABLE_PYG=1`

6) **Use Label Studio tooling instead of ad-hoc annotation edits**
   - `tools/label_studio/*` + `label_studio/` assets already cover import/export/fixes/validation

---

## 12) Cross-References (existing deep docs/reports)

These are already in-repo and should be treated as authoritative context (they’re not duplicated here):
- `docs/RUN_LOCAL.md` (runbook + truth metrics requirement)
- `docs/KNOWN_LIMITATIONS.md` (explicit refusal and failure modes)
- `docs/ARCHITECTURE.md` and `docs/METHODOLOGY.md` (design intent)
- `reports/CODEBASE_AUDIT_REPORT.md` (prior audit notes)
- `reports/QBM_DATA_ARCHITECTURE_AND_DOMAIN_AUDIT.md` (data architecture risks)
