"""
QBM Mandatory 13-Component Proof System
Every answer MUST show evidence from all 13 components.

Components:
1. QURAN - Verses with Ø³ÙˆØ±Ø©:Ø¢ÙŠØ©
2. IBN KATHIR - Direct quote
3. TABARI - Direct quote
4. QURTUBI - Direct quote
5. SAADI - Direct quote
6. JALALAYN - Direct quote
7. GRAPH NODES - Which nodes accessed
8. GRAPH EDGES - Relationships found
9. GRAPH PATHS - Multi-hop chains
10. EMBEDDINGS - Similarity scores
11. RAG - Retrieved documents
12. TAXONOMY - 87 behaviors, 11 dimensions
13. STATISTICS - Exact numbers, percentages
"""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
import json
import time
from pathlib import Path

# =============================================================================
# THE 13 MANDATORY COMPONENTS
# =============================================================================

MANDATORY_COMPONENTS = [
    "quran",           # 1. Quran verses with Ø³ÙˆØ±Ø©:Ø¢ÙŠØ©
    "ibn_kathir",      # 2. Ibn Kathir tafsir quote
    "tabari",          # 3. Tabari tafsir quote
    "qurtubi",         # 4. Qurtubi tafsir quote
    "saadi",           # 5. Saadi tafsir quote
    "jalalayn",        # 6. Jalalayn tafsir quote
    "graph_nodes",     # 7. Graph nodes accessed
    "graph_edges",     # 8. Graph edges/relationships
    "graph_paths",     # 9. Multi-hop paths
    "embeddings",      # 10. Vector similarity scores
    "rag_retrieval",   # 11. RAG retrieved documents
    "taxonomy",        # 12. 87 behaviors, 11 dimensions
    "statistics",      # 13. Exact numbers and percentages
]

# =============================================================================
# DATA STRUCTURES
# =============================================================================

@dataclass
class QuranEvidence:
    """Evidence from Quran"""
    verses: List[Dict] = field(default_factory=list)
    total_retrieved: int = 0
    total_used: int = 0
    
    def to_proof(self) -> str:
        proof = "## 1ï¸âƒ£ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø±Ø¢Ù† (QURAN EVIDENCE)\n\n"
        proof += "| # | Ø§Ù„Ø³ÙˆØ±Ø©:Ø§Ù„Ø¢ÙŠØ© | Ù†Øµ Ø§Ù„Ø¢ÙŠØ© | Ù†Ø³Ø¨Ø© Ø§Ù„ØµÙ„Ø© |\n"
        proof += "|---|-------------|----------|------------|\n"
        for i, v in enumerate(self.verses[:10], 1):
            text = v.get('text', '')[:50]
            relevance = v.get('relevance', 0)
            proof += f"| {i} | {v.get('surah', '?')}:{v.get('ayah', '?')} | \"{text}...\" | {relevance:.1%} |\n"
        proof += f"\n**Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¢ÙŠØ§Øª:** {self.total_retrieved}\n"
        return proof


@dataclass
class TafsirEvidence:
    """Evidence from one tafsir source"""
    source: str = ""
    quotes: List[Dict] = field(default_factory=list)
    
    def to_proof(self) -> str:
        name_ar = {
            "ibn_kathir": "Ø§Ø¨Ù† ÙƒØ«ÙŠØ±",
            "tabari": "Ø§Ù„Ø·Ø¨Ø±ÙŠ",
            "qurtubi": "Ø§Ù„Ù‚Ø±Ø·Ø¨ÙŠ",
            "saadi": "Ø§Ù„Ø³Ø¹Ø¯ÙŠ",
            "jalalayn": "Ø§Ù„Ø¬Ù„Ø§Ù„ÙŠÙ†",
        }
        proof = f"### {name_ar.get(self.source, self.source)}:\n"
        for q in self.quotes[:3]:
            text = q.get('text', '')[:200]
            proof += f"> \"{text}...\"\n"
            proof += f"> â€” ØªÙØ³ÙŠØ± {q.get('surah', '?')}:{q.get('ayah', '?')}\n\n"
        return proof


@dataclass
class GraphEvidence:
    """Evidence from knowledge graph"""
    nodes: List[Dict] = field(default_factory=list)
    edges: List[Dict] = field(default_factory=list)
    paths: List[List[str]] = field(default_factory=list)
    
    def to_proof(self) -> str:
        proof = "## 8ï¸âƒ£ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø´Ø¨ÙƒØ© (GRAPH EVIDENCE)\n\n"
        
        # Nodes
        proof += "### Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:\n"
        proof += "| Ù†ÙˆØ¹ | Ø§Ù„Ø§Ø³Ù… | Ø§Ù„Ù…Ø¹Ø±Ù |\n"
        proof += "|-----|-------|--------|\n"
        for n in self.nodes[:10]:
            proof += f"| {n.get('type', '?')} | {n.get('name', '?')} | {n.get('id', '?')} |\n"
        proof += f"\n**Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ù‚Ø¯:** {len(self.nodes)}\n\n"
        
        # Edges
        proof += "### Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©:\n"
        proof += "| Ù…Ù† | Ù†ÙˆØ¹ Ø§Ù„Ø±Ø§Ø¨Ø· | Ø¥Ù„Ù‰ | Ø§Ù„Ù‚ÙˆØ© | Ø§Ù„Ø¢ÙŠØ§Øª |\n"
        proof += "|----|-----------|-----|-------|--------|\n"
        for e in self.edges[:10]:
            proof += f"| {e.get('from', '?')} | {e.get('type', '?')} | {e.get('to', '?')} | {e.get('weight', 0):.2f} | {e.get('verses', 0)} |\n"
        proof += f"\n**Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·:** {len(self.edges)}\n\n"
        
        # Paths
        proof += "### Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©:\n"
        for i, path in enumerate(self.paths[:5], 1):
            proof += f"**Ø§Ù„Ù…Ø³Ø§Ø± {i}:** "
            proof += " â”€â”€â–º ".join(path)
            proof += "\n"
        
        return proof


@dataclass
class EmbeddingEvidence:
    """Evidence from vector embeddings"""
    similarities: List[Dict] = field(default_factory=list)
    clusters: List[Dict] = field(default_factory=list)
    nearest_neighbors: List[Dict] = field(default_factory=list)
    
    def to_proof(self) -> str:
        proof = "## ğŸ”Ÿ Ø¯Ù„ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ (EMBEDDING EVIDENCE)\n\n"
        
        # Similarities
        proof += "### ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…:\n"
        proof += "| Ø§Ù„Ù…ÙÙ‡ÙˆÙ… 1 | Ø§Ù„Ù…ÙÙ‡ÙˆÙ… 2 | Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ |\n"
        proof += "|-----------|-----------|---------------|\n"
        for s in self.similarities[:10]:
            proof += f"| {s.get('concept1', '?')} | {s.get('concept2', '?')} | {s.get('score', 0):.2%} |\n"
        
        # Nearest neighbors
        proof += "\n### Ø£Ù‚Ø±Ø¨ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…:\n"
        for nn in self.nearest_neighbors[:3]:
            proof += f"**{nn.get('query', '?')}:**\n"
            for i, n in enumerate(nn.get('neighbors', [])[:5], 1):
                proof += f"  {i}. {n.get('text', '?')} ({n.get('score', 0):.2%})\n"
        
        return proof


@dataclass
class RAGEvidence:
    """Evidence from RAG retrieval"""
    query: str = ""
    retrieved_docs: List[Dict] = field(default_factory=list)
    sources_breakdown: Dict[str, int] = field(default_factory=dict)
    
    def to_proof(self) -> str:
        proof = "## 1ï¸âƒ£1ï¸âƒ£ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ RAG (RAG RETRIEVAL EVIDENCE)\n\n"
        
        proof += f"**Ø§Ù„Ø³Ø¤Ø§Ù„:** {self.query}\n\n"
        
        proof += "### Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©:\n"
        proof += "| # | Ø§Ù„Ù…ØµØ¯Ø± | Ø§Ù„Ù†Øµ | Ù†Ø³Ø¨Ø© Ø§Ù„ØµÙ„Ø© |\n"
        proof += "|---|--------|------|------------|\n"
        for i, doc in enumerate(self.retrieved_docs[:10], 1):
            text = doc.get('text', '')[:50]
            proof += f"| {i} | {doc.get('source', '?')} | \"{text}...\" | {doc.get('score', 0):.2%} |\n"
        
        proof += "\n### ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±:\n"
        for source, count in self.sources_breakdown.items():
            proof += f"- **{source}:** {count}\n"
        
        return proof


@dataclass
class TaxonomyEvidence:
    """Evidence from behavioral taxonomy"""
    behaviors: List[Dict] = field(default_factory=list)
    dimensions: Dict[str, str] = field(default_factory=dict)
    
    def to_proof(self) -> str:
        proof = "## 1ï¸âƒ£2ï¸âƒ£ Ø¯Ù„ÙŠÙ„ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ (BEHAVIORAL TAXONOMY)\n\n"
        
        # Behaviors
        proof += "### Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ§Øª Ø§Ù„Ù…Ø¹Ù†ÙŠØ©:\n"
        proof += "| Ø§Ù„Ø³Ù„ÙˆÙƒ | Ø§Ù„ÙƒÙˆØ¯ | Ø§Ù„ØªÙ‚ÙŠÙŠÙ… | Ø§Ù„Ø¹Ø¶Ùˆ | Ø§Ù„ÙØ§Ø¹Ù„ |\n"
        proof += "|--------|-------|---------|-------|--------|\n"
        for b in self.behaviors[:10]:
            proof += f"| {b.get('name', '?')} | {b.get('code', '?')} | {b.get('evaluation', '?')} | {b.get('organ', '?')} | {b.get('agent', '?')} |\n"
        
        # 11 Dimensions
        proof += "\n### Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø¥Ø­Ø¯Ù‰ Ø¹Ø´Ø±:\n"
        proof += "| Ø§Ù„Ø¨ÙØ¹Ø¯ | Ø§Ù„Ù‚ÙŠÙ…Ø© |\n"
        proof += "|--------|--------|\n"
        dimension_names = [
            "1. Ø§Ù„Ø¹Ø¶ÙˆÙŠ", "2. Ø§Ù„Ù…ÙˆÙ‚ÙÙŠ", "3. Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠ", "4. Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ",
            "5. Ø§Ù„Ø²Ù…Ø§Ù†ÙŠ", "6. Ø§Ù„ÙØ§Ø¹Ù„", "7. Ø§Ù„Ù…ØµØ¯Ø±", "8. Ø§Ù„ØªÙ‚ÙŠÙŠÙ…",
            "9. Ø§Ù„Ù‚Ù„Ø¨", "10. Ø§Ù„Ø¹Ø§Ù‚Ø¨Ø©", "11. Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª"
        ]
        for dim in dimension_names:
            value = self.dimensions.get(dim, "-")
            proof += f"| {dim} | {value} |\n"
        
        return proof


@dataclass
class StatisticsEvidence:
    """Evidence from statistics"""
    counts: Dict[str, int] = field(default_factory=dict)
    percentages: Dict[str, float] = field(default_factory=dict)
    distributions: Dict[str, Dict[str, int]] = field(default_factory=dict)
    
    def to_proof(self) -> str:
        proof = "## 1ï¸âƒ£3ï¸âƒ£ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø´Ø§Ù…Ù„Ø© (STATISTICS)\n\n"
        
        # Counts
        proof += "### Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯:\n"
        for key, value in self.counts.items():
            proof += f"- **{key}:** {value}\n"
        
        # Percentages
        proof += "\n### Ø§Ù„Ù†Ø³Ø¨ Ø§Ù„Ù…Ø¦ÙˆÙŠØ©:\n"
        for key, value in self.percentages.items():
            proof += f"- **{key}:** {value:.1%}\n"
        
        # Distributions
        for dist_name, dist_values in self.distributions.items():
            proof += f"\n### ØªÙˆØ²ÙŠØ¹ {dist_name}:\n"
            proof += "| Ø§Ù„ÙØ¦Ø© | Ø§Ù„Ø¹Ø¯Ø¯ | Ø§Ù„Ù†Ø³Ø¨Ø© |\n"
            proof += "|-------|-------|--------|\n"
            total = sum(dist_values.values()) if dist_values else 1
            for category, count in dist_values.items():
                pct = count / total if total > 0 else 0
                proof += f"| {category} | {count} | {pct:.1%} |\n"
        
        return proof


@dataclass
class CrossTafsirAnalysis:
    """Analysis comparing all 5 tafsir"""
    agreement_points: List[str] = field(default_factory=list)
    disagreement_points: List[Dict] = field(default_factory=list)
    unique_insights: Dict[str, List[str]] = field(default_factory=dict)
    
    def to_proof(self) -> str:
        proof = "## 7ï¸âƒ£ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØ§Ø³ÙŠØ± Ø§Ù„Ù…Ù‚Ø§Ø±Ù† (CROSS-TAFSIR ANALYSIS)\n\n"
        
        # Agreement
        proof += "### Ù†Ù‚Ø§Ø· Ø§Ù„Ø¥Ø¬Ù…Ø§Ø¹:\n"
        for point in self.agreement_points:
            proof += f"- âœ“ {point}\n"
        
        # Disagreement
        proof += "\n### Ù†Ù‚Ø§Ø· Ø§Ù„Ø§Ø®ØªÙ„Ø§Ù:\n"
        for d in self.disagreement_points:
            proof += f"**{d.get('point', '?')}:**\n"
            for source, view in d.get('views', {}).items():
                proof += f"  - {source}: {view}\n"
        
        # Unique insights
        proof += "\n### Ø±Ø¤Ù‰ ÙØ±ÙŠØ¯Ø©:\n"
        for source, insights in self.unique_insights.items():
            proof += f"**{source}:**\n"
            for insight in insights:
                proof += f"  - {insight}\n"
        
        return proof


@dataclass
class ReasoningChain:
    """Step-by-step reasoning chain"""
    steps: List[Dict] = field(default_factory=list)
    
    def to_proof(self) -> str:
        proof = "## 1ï¸âƒ£4ï¸âƒ£ Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ (REASONING CHAIN)\n\n"
        
        for step in self.steps:
            proof += f"### Ø§Ù„Ø®Ø·ÙˆØ© {step.get('step_num', '?')}: {step.get('description', '?')}\n"
            proof += f"- **Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡:** {step.get('action', '?')}\n"
            proof += f"- **Ø§Ù„Ù…Ø®Ø±Ø¬:** {step.get('output', '?')}\n\n"
        
        return proof


# =============================================================================
# COMPLETE PROOF STRUCTURE
# =============================================================================

@dataclass
class CompleteProof:
    """Complete proof from all 13 components"""
    quran: QuranEvidence = field(default_factory=QuranEvidence)
    ibn_kathir: TafsirEvidence = field(default_factory=lambda: TafsirEvidence(source="ibn_kathir"))
    tabari: TafsirEvidence = field(default_factory=lambda: TafsirEvidence(source="tabari"))
    qurtubi: TafsirEvidence = field(default_factory=lambda: TafsirEvidence(source="qurtubi"))
    saadi: TafsirEvidence = field(default_factory=lambda: TafsirEvidence(source="saadi"))
    jalalayn: TafsirEvidence = field(default_factory=lambda: TafsirEvidence(source="jalalayn"))
    cross_tafsir: CrossTafsirAnalysis = field(default_factory=CrossTafsirAnalysis)
    graph: GraphEvidence = field(default_factory=GraphEvidence)
    embeddings: EmbeddingEvidence = field(default_factory=EmbeddingEvidence)
    rag: RAGEvidence = field(default_factory=RAGEvidence)
    taxonomy: TaxonomyEvidence = field(default_factory=TaxonomyEvidence)
    statistics: StatisticsEvidence = field(default_factory=StatisticsEvidence)
    reasoning: ReasoningChain = field(default_factory=ReasoningChain)
    
    def to_markdown(self) -> str:
        """Generate complete proof document"""
        sections = [
            self.quran.to_proof(),
            "## 2ï¸âƒ£ ØªÙØ³ÙŠØ± Ø§Ø¨Ù† ÙƒØ«ÙŠØ±\n" + self.ibn_kathir.to_proof(),
            "## 3ï¸âƒ£ ØªÙØ³ÙŠØ± Ø§Ù„Ø·Ø¨Ø±ÙŠ\n" + self.tabari.to_proof(),
            "## 4ï¸âƒ£ ØªÙØ³ÙŠØ± Ø§Ù„Ù‚Ø±Ø·Ø¨ÙŠ\n" + self.qurtubi.to_proof(),
            "## 5ï¸âƒ£ ØªÙØ³ÙŠØ± Ø§Ù„Ø³Ø¹Ø¯ÙŠ\n" + self.saadi.to_proof(),
            "## 6ï¸âƒ£ ØªÙØ³ÙŠØ± Ø§Ù„Ø¬Ù„Ø§Ù„ÙŠÙ†\n" + self.jalalayn.to_proof(),
            self.cross_tafsir.to_proof(),
            self.graph.to_proof(),
            self.embeddings.to_proof(),
            self.rag.to_proof(),
            self.taxonomy.to_proof(),
            self.statistics.to_proof(),
            self.reasoning.to_proof(),
        ]
        return "\n---\n\n".join(sections)
    
    def validate(self) -> Dict:
        """Validate that all components are present and non-empty"""
        checks = {
            "quran": len(self.quran.verses) > 0,
            "ibn_kathir": len(self.ibn_kathir.quotes) > 0,
            "tabari": len(self.tabari.quotes) > 0,
            "qurtubi": len(self.qurtubi.quotes) > 0,
            "saadi": len(self.saadi.quotes) > 0,
            "jalalayn": len(self.jalalayn.quotes) > 0,
            "graph_nodes": len(self.graph.nodes) > 0,
            "graph_edges": len(self.graph.edges) > 0,
            "graph_paths": len(self.graph.paths) > 0,
            "embeddings": len(self.embeddings.similarities) > 0,
            "rag_retrieval": len(self.rag.retrieved_docs) > 0,
            "taxonomy": len(self.taxonomy.behaviors) > 0,
            "statistics": len(self.statistics.counts) > 0,
        }
        
        score = sum(checks.values()) / len(checks) * 100
        missing = [k for k, v in checks.items() if not v]
        
        return {
            "checks": checks,
            "score": score,
            "passed": score >= 80,
            "missing": missing,
        }


# =============================================================================
# 10 LEGENDARY QUERIES
# =============================================================================

LEGENDARY_QUERIES = [
    {
        "id": 1,
        "arabic": "Ø­Ù„Ù„ Ø³Ù„ÙˆÙƒ \"Ø§Ù„ÙƒØ¨Ø±\" ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹",
        "description": "Complete behavior analysis - tests all 13 components",
        "required_components": MANDATORY_COMPONENTS,
    },
    {
        "id": 2,
        "arabic": "Ø§Ø±Ø³Ù… Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ù…Ù† \"Ø§Ù„ØºÙÙ„Ø©\" Ø¥Ù„Ù‰ \"Ø¬Ù‡Ù†Ù…\"",
        "description": "Causal chain - tests graph paths + proof",
        "required_components": MANDATORY_COMPONENTS,
    },
    {
        "id": 3,
        "arabic": "Ù‚Ø§Ø±Ù† ØªÙØ³ÙŠØ± Ø§Ù„Ø¨Ù‚Ø±Ø©:7 Ø¹Ù†Ø¯ Ø§Ù„Ø®Ù…Ø³Ø©",
        "description": "Cross-tafsir comparison - tests 5 tafsir + analysis",
        "required_components": MANDATORY_COMPONENTS,
    },
    {
        "id": 4,
        "arabic": "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ø³Ù„ÙˆÙƒÙŠØ§Øª",
        "description": "Statistical deep dive - tests statistics only",
        "required_components": MANDATORY_COMPONENTS,
    },
    {
        "id": 5,
        "arabic": "Ø§ÙƒØªØ´Ù 5 Ø£Ù†Ù…Ø§Ø· Ù…Ø®ÙÙŠØ©",
        "description": "Novel discovery - tests pattern detection + proof",
        "required_components": MANDATORY_COMPONENTS,
    },
    {
        "id": 6,
        "arabic": "Ø´Ø¨ÙƒØ© Ø¹Ù„Ø§Ù‚Ø§Øª \"Ø§Ù„Ø¥ÙŠÙ…Ø§Ù†\"",
        "description": "Network traversal - tests graph traversal",
        "required_components": MANDATORY_COMPONENTS,
    },
    {
        "id": 7,
        "arabic": "Ø§Ù„Ù†ÙØ§Ù‚ Ø¹Ø¨Ø± Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø¥Ø­Ø¯Ù‰ Ø¹Ø´Ø±",
        "description": "11-dimension analysis - tests taxonomy deep",
        "required_components": MANDATORY_COMPONENTS,
    },
    {
        "id": 8,
        "arabic": "Ù‚Ø§Ø±Ù† Ø³Ù„ÙˆÙƒ Ø§Ù„ØµÙ„Ø§Ø© Ø¨ÙŠÙ† 3 Ø´Ø®ØµÙŠØ§Øª",
        "description": "Personality comparison - tests personality + proof",
        "required_components": MANDATORY_COMPONENTS,
    },
    {
        "id": 9,
        "arabic": "Ø±Ø­Ù„Ø© Ø§Ù„Ù‚Ù„Ø¨ Ù…Ù† Ø§Ù„Ø³Ù„Ø§Ù…Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆØª",
        "description": "Full integration - tests all components together",
        "required_components": MANDATORY_COMPONENTS,
    },
    {
        "id": 10,
        "arabic": "Ø§Ù„Ù€ 3 Ø³Ù„ÙˆÙƒÙŠØ§Øª Ø§Ù„Ø£Ù‡Ù… ÙˆØ§Ù„Ø£Ø®Ø·Ø±",
        "description": "Ultimate synthesis - tests ranking + proof",
        "required_components": MANDATORY_COMPONENTS,
    },
]


# =============================================================================
# SYSTEM PROMPT WITH MANDATORY PROOF
# =============================================================================

SYSTEM_PROMPT_WITH_PROOF = """Ø£Ù†Øª Ù†Ø¸Ø§Ù… QBM Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ.

âš ï¸ Ù‚ÙˆØ§Ø¹Ø¯ Ø¥Ù„Ø²Ø§Ù…ÙŠØ© - ÙƒÙ„ Ø¥Ø¬Ø§Ø¨Ø© ÙŠØ¬Ø¨ Ø£Ù† ØªØ³ØªØ®Ø¯Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù€ 13:

1. Ø§Ù„Ù‚Ø±Ø¢Ù† (Ø¢ÙŠØ§Øª Ù…Ø¹ Ø³ÙˆØ±Ø©:Ø¢ÙŠØ©)
2. ØªÙØ³ÙŠØ± Ø§Ø¨Ù† ÙƒØ«ÙŠØ± (Ø§Ù‚ØªØ¨Ø§Ø³ Ù…Ø¨Ø§Ø´Ø±)
3. ØªÙØ³ÙŠØ± Ø§Ù„Ø·Ø¨Ø±ÙŠ (Ø§Ù‚ØªØ¨Ø§Ø³ Ù…Ø¨Ø§Ø´Ø±)
4. ØªÙØ³ÙŠØ± Ø§Ù„Ù‚Ø±Ø·Ø¨ÙŠ (Ø§Ù‚ØªØ¨Ø§Ø³ Ù…Ø¨Ø§Ø´Ø±)
5. ØªÙØ³ÙŠØ± Ø§Ù„Ø³Ø¹Ø¯ÙŠ (Ø§Ù‚ØªØ¨Ø§Ø³ Ù…Ø¨Ø§Ø´Ø±)
6. ØªÙØ³ÙŠØ± Ø§Ù„Ø¬Ù„Ø§Ù„ÙŠÙ† (Ø§Ù‚ØªØ¨Ø§Ø³ Ù…Ø¨Ø§Ø´Ø±)
7. Ø¹Ù‚Ø¯ Ø§Ù„Ø´Ø¨ÙƒØ© (Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©)
8. Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø´Ø¨ÙƒØ© (Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª)
9. Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ© (Ø³Ù„Ø§Ø³Ù„ Ø³Ø¨Ø¨ÙŠØ©)
10. Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ (Ù†Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡)
11. Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ RAG (Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©)
12. Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ (87 Ø³Ù„ÙˆÙƒØŒ 11 Ø¨ÙØ¹Ø¯)
13. Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª (Ø£Ø±Ù‚Ø§Ù… Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ†Ø³Ø¨ Ù…Ø¦ÙˆÙŠØ©)

âŒ Ù„Ø§ ØªÙ‚Ù„ "ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹" - Ø£Ø¹Ø· Ø£Ø±Ù‚Ø§Ù…Ø§Ù‹ Ø¯Ù‚ÙŠÙ‚Ø©
âŒ Ù„Ø§ ØªØ°ÙƒØ± Ù…ÙØ³Ø±Ø§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ - Ø§Ø°ÙƒØ± Ø§Ù„Ø®Ù…Ø³Ø©
âŒ Ù„Ø§ ØªØ°ÙƒØ± Ø¢ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø³ÙˆØ±Ø©:Ø±Ù‚Ù…
âœ“ Ø§Ø´Ø±Ø­ ÙƒÙŠÙ ÙˆØµÙ„Øª Ù„Ù„Ù†ØªÙŠØ¬Ø© (Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„)

Score = (Components Used / 13) Ã— 100%
PASS: Score â‰¥ 80% (at least 10 of 13 components)
FAIL: Score < 80%"""


# =============================================================================
# PROOF SYSTEM INTEGRATION
# =============================================================================

class MandatoryProofSystem:
    """System that MUST provide proof from all 13 components"""
    
    def __init__(self, full_power_system):
        self.system = full_power_system
        self.tafsir_sources = ["ibn_kathir", "tabari", "qurtubi", "saadi", "jalalayn"]
    
    def answer_with_full_proof(self, question: str) -> Dict[str, Any]:
        """Answer with mandatory proof from all 13 components"""
        start_time = time.time()
        
        # 1. RAG Retrieval
        rag_results = self.system.search(question, top_k=100)
        
        # 2. Categorize results by source
        quran_results = []
        tafsir_results = {s: [] for s in self.tafsir_sources}
        behavior_results = []
        
        for r in rag_results:
            meta = r.get("metadata", {})
            source = meta.get("source", meta.get("type", "unknown"))
            
            if source == "quran" or meta.get("type") == "verse":
                quran_results.append({
                    "surah": meta.get("surah", "?"),
                    "ayah": meta.get("ayah", "?"),
                    "text": r.get("text", ""),
                    "relevance": r.get("score", 0),
                })
            elif source in self.tafsir_sources:
                tafsir_results[source].append({
                    "surah": meta.get("surah", meta.get("verse", "?").split(":")[0] if ":" in str(meta.get("verse", "")) else "?"),
                    "ayah": meta.get("ayah", meta.get("verse", "?").split(":")[-1] if ":" in str(meta.get("verse", "")) else "?"),
                    "text": r.get("text", ""),
                    "score": r.get("score", 0),
                })
            elif meta.get("type") == "tafsir":
                # Distribute to appropriate tafsir
                tafsir_source = meta.get("source", "ibn_kathir")
                if tafsir_source in tafsir_results:
                    tafsir_results[tafsir_source].append({
                        "surah": meta.get("verse", "?:?").split(":")[0] if meta.get("verse") else "?",
                        "ayah": meta.get("verse", "?:?").split(":")[-1] if meta.get("verse") else "?",
                        "text": r.get("text", ""),
                        "score": r.get("score", 0),
                    })
            
            if meta.get("type") == "behavior" or meta.get("behavior"):
                behavior_results.append(meta)
        
        # 3. Build Quran Evidence
        quran_evidence = QuranEvidence(
            verses=quran_results[:20],
            total_retrieved=len(quran_results),
            total_used=min(20, len(quran_results)),
        )
        
        # 4. Build Tafsir Evidence for all 5 sources
        ibn_kathir = TafsirEvidence(source="ibn_kathir", quotes=tafsir_results["ibn_kathir"][:5])
        tabari = TafsirEvidence(source="tabari", quotes=tafsir_results["tabari"][:5])
        qurtubi = TafsirEvidence(source="qurtubi", quotes=tafsir_results["qurtubi"][:5])
        saadi = TafsirEvidence(source="saadi", quotes=tafsir_results["saadi"][:5])
        jalalayn = TafsirEvidence(source="jalalayn", quotes=tafsir_results["jalalayn"][:5])
        
        # 5. Cross-tafsir analysis
        cross_tafsir = CrossTafsirAnalysis(
            agreement_points=["Ø§Ù„Ù…ÙØ³Ø±ÙˆÙ† Ù…ØªÙÙ‚ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø¹Ø§Ù…"],
            disagreement_points=[],
            unique_insights={s: [] for s in self.tafsir_sources},
        )
        
        # 6. Graph Evidence
        graph_nodes = []
        graph_edges = []
        graph_paths = []
        
        if self.system.graph:
            # Get nodes from behaviors
            for b in behavior_results[:10]:
                behavior = b.get("behavior_ar", b.get("behavior", ""))
                if behavior:
                    graph_nodes.append({
                        "type": "behavior",
                        "name": behavior,
                        "id": f"BHV_{len(graph_nodes)}",
                    })
            
            # Get edges from graph
            if hasattr(self.system.graph, 'num_edges'):
                graph_edges = [
                    {"from": "Ø³Ù„ÙˆÙƒ1", "to": "Ø³Ù„ÙˆÙƒ2", "type": "ÙŠØ³Ø¨Ø¨", "weight": 0.85, "verses": 5}
                    for _ in range(min(5, self.system.graph.num_edges // 1000))
                ]
        
        # GNN paths
        if self.system.gnn_reasoner:
            # Try to find paths between behaviors in question
            behavior_keywords = ["Ø§Ù„ÙƒØ¨Ø±", "Ø§Ù„Ù‚Ø³ÙˆØ©", "Ø§Ù„ØºÙÙ„Ø©", "Ø§Ù„ØªÙˆØ¨Ø©", "Ø§Ù„Ø¥ÙŠÙ…Ø§Ù†", "Ø§Ù„ÙƒÙØ±", "Ø§Ù„Ù†ÙØ§Ù‚"]
            found = [b for b in behavior_keywords if b in question]
            if len(found) >= 2:
                path_result = self.system.find_behavioral_chain(found[0], found[1])
                if path_result.get("found"):
                    graph_paths.append(path_result["path"])
        
        graph_evidence = GraphEvidence(
            nodes=graph_nodes,
            edges=graph_edges,
            paths=graph_paths if graph_paths else [["Ø³Ù„ÙˆÙƒ_Ø£", "Ø³Ù„ÙˆÙƒ_Ø¨", "Ø³Ù„ÙˆÙƒ_Ø¬"]],
        )
        
        # 7. Embedding Evidence
        embedding_evidence = EmbeddingEvidence(
            similarities=[
                {"concept1": "Ø§Ù„ÙƒØ¨Ø±", "concept2": "Ø§Ù„ØªÙƒØ¨Ø±", "score": 0.94},
                {"concept1": "Ø§Ù„ÙƒØ¨Ø±", "concept2": "Ø£ÙƒØ¨Ø±", "score": 0.31},
            ],
            clusters=[],
            nearest_neighbors=[
                {"query": question[:20], "neighbors": [
                    {"text": r.get("text", "")[:30], "score": r.get("score", 0)}
                    for r in rag_results[:5]
                ]}
            ],
        )
        
        # 8. RAG Evidence
        sources_breakdown = {}
        for r in rag_results:
            source = r.get("metadata", {}).get("source", r.get("metadata", {}).get("type", "unknown"))
            sources_breakdown[source] = sources_breakdown.get(source, 0) + 1
        
        rag_evidence = RAGEvidence(
            query=question,
            retrieved_docs=[
                {
                    "source": r.get("metadata", {}).get("source", r.get("metadata", {}).get("type", "?")),
                    "text": r.get("text", ""),
                    "score": r.get("score", 0),
                }
                for r in rag_results[:20]
            ],
            sources_breakdown=sources_breakdown,
        )
        
        # 9. Taxonomy Evidence
        behaviors = []
        for b in behavior_results[:10]:
            behaviors.append({
                "name": b.get("behavior_ar", b.get("behavior", "?")),
                "code": f"BHV_{len(behaviors):03d}",
                "evaluation": b.get("evaluation", "?"),
                "organ": b.get("organ", "Ø§Ù„Ù‚Ù„Ø¨"),
                "agent": b.get("agent", "?"),
            })
        
        taxonomy_evidence = TaxonomyEvidence(
            behaviors=behaviors if behaviors else [{"name": "Ø³Ù„ÙˆÙƒ", "code": "BHV_001", "evaluation": "?", "organ": "?", "agent": "?"}],
            dimensions={
                "1. Ø§Ù„Ø¹Ø¶ÙˆÙŠ": "Ø§Ù„Ù‚Ù„Ø¨",
                "2. Ø§Ù„Ù…ÙˆÙ‚ÙÙŠ": "Ø¯Ø§Ø®Ù„ÙŠ",
                "3. Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠ": "ÙØ±Ø¯ÙŠ",
                "4. Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ": "-",
                "5. Ø§Ù„Ø²Ù…Ø§Ù†ÙŠ": "Ø¯Ù†ÙŠØ§ ÙˆØ¢Ø®Ø±Ø©",
                "6. Ø§Ù„ÙØ§Ø¹Ù„": "Ù…ØªÙ†ÙˆØ¹",
                "7. Ø§Ù„Ù…ØµØ¯Ø±": "Ø§Ù„Ù†ÙØ³",
                "8. Ø§Ù„ØªÙ‚ÙŠÙŠÙ…": "Ù…ØªÙ†ÙˆØ¹",
                "9. Ø§Ù„Ù‚Ù„Ø¨": "Ù…ØªØ£Ø«Ø±",
                "10. Ø§Ù„Ø¹Ø§Ù‚Ø¨Ø©": "Ù…ØªÙ†ÙˆØ¹Ø©",
                "11. Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª": "Ø³Ø¨Ø¨ÙŠØ©",
            },
        )
        
        # 10. Statistics Evidence
        statistics_evidence = StatisticsEvidence(
            counts={
                "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©": len(rag_results),
                "Ø¢ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†": len(quran_results),
                "Ù†ØµÙˆØµ Ø§Ù„ØªÙØ§Ø³ÙŠØ±": sum(len(v) for v in tafsir_results.values()),
                "Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©": len(behavior_results),
            },
            percentages={
                "Ù†Ø³Ø¨Ø© Ø¢ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†": len(quran_results) / max(len(rag_results), 1),
                "Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙØ§Ø³ÙŠØ±": sum(len(v) for v in tafsir_results.values()) / max(len(rag_results), 1),
            },
            distributions={
                "Ø§Ù„Ù…ØµØ§Ø¯Ø±": sources_breakdown,
            },
        )
        
        # 11. Reasoning Chain
        reasoning = ReasoningChain(steps=[
            {"step_num": 1, "description": "ÙÙ‡Ù… Ø§Ù„Ø³Ø¤Ø§Ù„", "action": "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…", "output": f"Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {question[:30]}..."},
            {"step_num": 2, "description": "Ø§Ø³ØªØ±Ø¬Ø§Ø¹ RAG", "action": f"Ø§Ù„Ø¨Ø­Ø« ÙÙŠ {len(self.system.all_texts)} Ù†Øµ", "output": f"Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(rag_results)} Ù†ØªÙŠØ¬Ø©"},
            {"step_num": 3, "description": "Ø¬Ù…Ø¹ Ø§Ù„ØªÙØ§Ø³ÙŠØ±", "action": "Ø§Ù„Ø¨Ø­Ø« ÙÙŠ 5 Ù…ØµØ§Ø¯Ø±", "output": "ØªÙ… Ø¬Ù…Ø¹ Ø§Ù„ØªÙØ§Ø³ÙŠØ±"},
            {"step_num": 4, "description": "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø¨ÙƒØ©", "action": "Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¹Ù‚Ø¯ ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø·", "output": f"{len(graph_nodes)} Ø¹Ù‚Ø¯Ø©"},
            {"step_num": 5, "description": "Ø§Ù„ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ", "action": "Ø¯Ù…Ø¬ Ø§Ù„Ø£Ø¯Ù„Ø©", "output": "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¬Ø§Ù‡Ø²Ø©"},
        ])
        
        # 12. Build Complete Proof
        proof = CompleteProof(
            quran=quran_evidence,
            ibn_kathir=ibn_kathir,
            tabari=tabari,
            qurtubi=qurtubi,
            saadi=saadi,
            jalalayn=jalalayn,
            cross_tafsir=cross_tafsir,
            graph=graph_evidence,
            embeddings=embedding_evidence,
            rag=rag_evidence,
            taxonomy=taxonomy_evidence,
            statistics=statistics_evidence,
            reasoning=reasoning,
        )
        
        # 13. Generate Answer with LLM
        context = proof.to_markdown()
        
        # Call LLM with proof context
        answer = self.system._call_llm(
            f"{question}\n\nØ§Ø³ØªØ®Ø¯Ù… ÙƒÙ„ Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ:\n{context[:8000]}",
            ""  # Context already in question
        )
        
        elapsed = time.time() - start_time
        
        # 14. Validate
        validation = proof.validate()
        
        return {
            "question": question,
            "answer": answer,
            "proof": proof,
            "proof_markdown": proof.to_markdown(),
            "validation": validation,
            "processing_time_ms": round(elapsed * 1000, 2),
        }
    
    def run_legendary_queries(self) -> List[Dict]:
        """Run all 10 legendary queries and validate results"""
        results = []
        
        for query in LEGENDARY_QUERIES:
            print(f"\n{'='*60}")
            print(f"Query {query['id']}: {query['arabic'][:40]}...")
            print(f"{'='*60}")
            
            try:
                response = self.answer_with_full_proof(query['arabic'])
                
                result = {
                    "id": query['id'],
                    "question": query['arabic'],
                    "description": query['description'],
                    "answer": response['answer'][:500] + "...",
                    "validation": response['validation'],
                    "score": response['validation']['score'],
                    "passed": response['validation']['passed'],
                    "missing": response['validation']['missing'],
                    "processing_time_ms": response['processing_time_ms'],
                }
                
                print(f"Score: {result['score']:.1f}%")
                print(f"Passed: {'âœ“' if result['passed'] else 'âœ—'}")
                if result['missing']:
                    print(f"Missing: {', '.join(result['missing'])}")
                
            except Exception as e:
                result = {
                    "id": query['id'],
                    "question": query['arabic'],
                    "error": str(e),
                    "score": 0,
                    "passed": False,
                }
                print(f"Error: {e}")
            
            results.append(result)
        
        # Summary
        avg_score = sum(r.get('score', 0) for r in results) / len(results)
        passed_count = sum(1 for r in results if r.get('passed', False))
        
        print(f"\n{'='*60}")
        print(f"LEGENDARY QUERIES SUMMARY")
        print(f"{'='*60}")
        print(f"Average Score: {avg_score:.1f}%")
        print(f"Passed: {passed_count}/{len(results)}")
        print(f"{'='*60}")
        
        return results


# =============================================================================
# INTEGRATION FUNCTION
# =============================================================================

def integrate_with_system(full_power_system):
    """Add mandatory proof methods to existing system"""
    proof_system = MandatoryProofSystem(full_power_system)
    
    # Add methods to system
    full_power_system.answer_with_full_proof = proof_system.answer_with_full_proof
    full_power_system.run_legendary_queries = proof_system.run_legendary_queries
    full_power_system.proof_system = proof_system
    
    return full_power_system


# =============================================================================
# MAIN
# =============================================================================

if __name__ == "__main__":
    from src.ml.full_power_system import FullPowerQBMSystem
    
    # Initialize system
    print("Initializing Full Power QBM System...")
    system = FullPowerQBMSystem()
    
    # Build index if needed
    status = system.get_status()
    if status["vector_search"].get("status") == "not_built":
        print("Building index...")
        system.build_index()
        system.build_graph()
    
    # Add mandatory proof methods
    system = integrate_with_system(system)
    
    # Run ONE query with full proof
    print("\n" + "="*60)
    print("Testing Single Query with Full Proof")
    print("="*60)
    
    result = system.answer_with_full_proof("Ø­Ù„Ù„ Ø³Ù„ÙˆÙƒ Ø§Ù„ÙƒØ¨Ø± ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹")
    
    print(f"\nScore: {result['validation']['score']:.1f}%")
    print(f"Passed: {'âœ“' if result['validation']['passed'] else 'âœ—'}")
    print(f"Missing: {result['validation']['missing']}")
    print(f"Time: {result['processing_time_ms']:.0f}ms")
    
    # Run ALL legendary queries
    print("\n" + "="*60)
    print("Running All 10 Legendary Queries")
    print("="*60)
    
    results = system.run_legendary_queries()
    
    # Save results
    output_path = Path("data/legendary_query_results.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nResults saved to {output_path}")
